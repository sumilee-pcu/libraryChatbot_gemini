import os
import streamlit as st
import nest_asyncio

nest_asyncio.apply()

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory

__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
from langchain_chroma import Chroma



# ============================
# 1. API Key ì„¤ì •
# ============================
try:
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except:
    st.error("âš ï¸ GOOGLE_API_KEYë¥¼ Streamlit Secretsì— ì„¤ì •í•´ì£¼ì„¸ìš”!")
    st.stop()




# ============================
# 2. PDF ë¡œë“œ í•¨ìˆ˜
# ============================
@st.cache_resource
def load_and_split_pdf(file_path):
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()



# ============================
# 3. ì„ë² ë”© + Vector DB êµ¬ì¶•
# ============================
@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    split_docs = text_splitter.split_documents(_docs)

    st.info(f"ğŸ“„ {len(split_docs)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    persist_directory = "./chroma_db_marine_biodegradable"

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    vectorstore = Chroma.from_documents(
        split_docs,
        embeddings,
        persist_directory=persist_directory
    )

    st.success("ğŸŒŠ í•´ì–‘ ìƒë¶„í•´ ì‹ ì†Œì¬ Vector DB êµ¬ì¶• ì™„ë£Œ!")
    return vectorstore


@st.cache_resource
def get_vectorstore(_docs):
    persist_directory = "./chroma_db_marine_biodegradable"

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    if os.path.exists(persist_directory):
        return Chroma(
            persist_directory=persist_directory,
            embedding_function=embeddings
        )
    else:
        return create_vector_store(_docs)




# ============================
# 4. RAG êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
# ============================
@st.cache_resource
def initialize_components(selected_model):

    # ğŸ‘‰ ì´ PDF ê²½ë¡œë§Œ êµì²´í•˜ë©´ ë¨ (ì˜ˆ: PHA, PLA, í•´ì–‘ ë¯¸ìƒë¬¼ ê¸°ë°˜ ìƒë¶„í•´ ì—°êµ¬ PDF)
    file_path = r"/mnt/data/Review_of_recent_advances_in_the_biodegradability_.pdf"


    pages = load_and_split_pdf(file_path)
    vectorstore = get_vectorstore(pages)
    retriever = vectorstore.as_retriever()

    # ğŸ”µ ì§ˆë¬¸ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = """
    Reformulate the userâ€™s question into a standalone question 
    using the conversation history only for context. Do NOT answer.
    """

    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("history"),
        ("human", "{input}"),
    ])

    # ğŸ”µ ìƒë¶„í•´ ì‹ ì†Œì¬ Q&A í”„ë¡¬í”„íŠ¸
    qa_system_prompt = """
    ë‹¹ì‹ ì€ í•´ì–‘ í”Œë¼ìŠ¤í‹± ë¶„í•´ ì‹ ì†Œì¬(PHA, PLA, ë¯¸ìƒë¬¼ ê¸°ë°˜ í´ë¦¬ë¨¸ ë“±)ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” AI ì¡°êµì…ë‹ˆë‹¤.
    ì•„ë˜ ì œê³µëœ ì—°êµ¬ìë£Œì™€ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
    ì •ë³´ë¥¼ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ê³ , ì¶”ì¸¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    ë‹µë³€ì€ í•œêµ­ì–´ + ì¡´ëŒ“ë§ + ì´ëª¨ì§€ ì¡°í•©ì„ ìœ ì§€í•˜ì„¸ìš”.

    {context}
    """

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        MessagesPlaceholder("history"),
        ("human", "{input}"),
    ])

    llm = ChatGoogleGenerativeAI(
        model=selected_model,
        temperature=0.6,
        convert_system_message_to_human=True
    )

    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )

    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    rag_chain = create_retrieval_chain(
        history_aware_retriever,
        question_answer_chain
    )

    return rag_chain




# ============================
# 5. Streamlit UI
# ============================
st.header("ğŸŒŠ í•´ì–‘ í”Œë¼ìŠ¤í‹± ë¶„í•´ ì‹ ì†Œì¬ RAG ì±—ë´‡")

if not os.path.exists("./chroma_db_marine_biodegradable"):
    st.info("ğŸ”„ ì²« ì‹¤í–‰: PDF ì„ë² ë”© ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")

option = st.selectbox(
    "Select Gemini Model",
    ("gemini-2.0-flash-exp", "gemini-2.5-flash", "gemini-2.0-flash-lite"),
    index=0
)

with st.spinner("ğŸ”§ ì—°êµ¬ìë£Œ ë¡œë”© ë° ëª¨ë¸ ì´ˆê¸°í™” ì¤‘..."):
    rag_chain = initialize_components(option)

st.success("âœ… ì±—ë´‡ ì¤€ë¹„ ì™„ë£Œ!")




# ============================
# 6. ëŒ€í™” íˆìŠ¤í† ë¦¬ ë° RAG
# ============================
chat_history = StreamlitChatMessageHistory(key="chat_messages")

conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
)




# ============================
# 7. ê¸°ì¡´ íˆìŠ¤í† ë¦¬ ì¶œë ¥
# ============================
for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)




# ============================
# 8. ìœ ì € ì§ˆë¬¸ ì²˜ë¦¬
# ============================
if prompt := st.chat_input("í•´ì–‘ í”Œë¼ìŠ¤í‹± ë¶„í•´ ì‹ ì†Œì¬ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”! ğŸŒ±"):
    st.chat_message("human").write(prompt)

    with st.chat_message("ai"):
        with st.spinner("ğŸ” ìë£Œ ê²€ìƒ‰ ë° ë¶„ì„ ì¤‘..."):
            config = {"configurable": {"session_id": "any"}}

            response = conversational_rag_chain.invoke(
                {"input": prompt},
                config
            )

            st.write(response["answer"])

            with st.expander("ğŸ“„ ì°¸ê³  ë¬¸ì„œ ë³´ê¸°"):
                for doc in response["context"]:
                    st.markdown(doc.metadata['source'], help=doc.page_content)
SS
